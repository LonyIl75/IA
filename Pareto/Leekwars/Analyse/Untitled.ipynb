{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd87e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from filenameVerif import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f775b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from operator import sub,add\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6fdeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoaderFiles():\n",
    "    def __init__(self,pathFiles,batch_len =  5 ):\n",
    "        self.path_files = pathFiles\n",
    "        self.treated_files = []\n",
    "        self.not_treatedFiles = []\n",
    "        self.batch_len = batch_len\n",
    "        \n",
    "    def left_setDiff(lst_1,lst2):\n",
    "        return  list(set(lst_1) - set(lst2))\n",
    "        \n",
    "    def updateNotTreatedFiles(self):\n",
    "        lst_json = self.getCurFiles()\n",
    "        self.not_treatedFiles = BatchLoaderFiles.left_setDiff(lst_json,self.treated_files)\n",
    "        \n",
    "    def getCurFiles (self ):\n",
    "        lst_json = []\n",
    "        if( os.path.exists(self.path_files )) :\n",
    "            lst_json = [pos_json for pos_json in os.listdir(self.path_files) if pos_json.endswith('.json') and filename_isvalid(pos_json)]\n",
    "        return lst_json\n",
    "    \n",
    "    def updateTreatedFiles(self,lst_files_treated):\n",
    "        for f in lst_files_treated :\n",
    "            if( f in self.not_treatedFiles ):\n",
    "                self.treated_files.append(f)\n",
    "                self.not_treatedFiles.remove(f)\n",
    "                \n",
    "    def getNextBatch(self):\n",
    "        notTreated_len = len(self.not_treatedFiles) \n",
    "        if(notTreated_len <self.batch_len) : \n",
    "            self.updateNotTreatedFiles()\n",
    "        if( len(self.not_treatedFiles ) <self.batch_len ): \n",
    "            return self.not_treatedFiles\n",
    "        return self.not_treatedFiles[:self.batch_len]\n",
    "    \n",
    "                \n",
    "    \n",
    "                \n",
    "                \n",
    "    \n",
    "                \n",
    "    \n",
    "class Test():\n",
    "    def __init__ (self,_name_test , _expected , _result , f_eq = None ):\n",
    "        self.test_name = _name_test\n",
    "        self.expected = _expected \n",
    "        self.result = _result\n",
    "        self.f_eq = f_eq\n",
    "    def message_testFailed(self):\n",
    "        return \"TEST \" + self.test_name +\" FAILED \" \n",
    "    def message_diffExpectedResult(self):\n",
    "        return \"Return \" + str(self.result) + \" vs \" + \"Expected \" + str(self.expected )\n",
    "    \n",
    "    def message_testSuccess(self):\n",
    "        return \"TEST \" + self.test_name +\" PASSED \"\n",
    "    \n",
    "    \n",
    "    def print_resultTest( self   ):\n",
    "            if(self.result!=self.expected if self.f_eq is None  else not(self.f_eq ( self.result,self.expected))):\n",
    "                print(self.message_testFailed())\n",
    "                print(self.message_diffExpectedResult())\n",
    "            else :\n",
    "                print(self.message_testSuccess())\n",
    "    \n",
    "    \n",
    "\n",
    "class Mock_BatchLoader():\n",
    "    \n",
    "    def delete_content_files(str_path):\n",
    "        path_files = Path(str_path)\n",
    "        path_files = Test_BatchLoader.rm_ifExist(path_files)\n",
    "        \n",
    "    def mock_create_files(str_path ,lst_files):\n",
    "        path_files = Path(str_path)\n",
    "        if not(path_files.exists() and path_files.is_dir()):\n",
    "            os.mkdir(str_path)\n",
    "        for str_file in lst_files:\n",
    "            with open(os.path.join(str_path,str_file), 'w') as f:\n",
    "                f.write(\"{}\")\n",
    "    \n",
    "class Test_BatchLoader(Test):\n",
    "    def __init__(self):\n",
    "        name_test , result,expected = Test_BatchLoader.test_leftSetDiff()\n",
    "        test_setDiff = Test(name_test,result,expected)\n",
    "        test_setDiff.print_resultTest()\n",
    "        \n",
    "        name_test , result,expected = Test_BatchLoader.test_getCurFiles()\n",
    "        test_setDiff = Test(name_test,result,expected,(lambda x,y : sorted(x)==sorted(y)))\n",
    "        test_setDiff.print_resultTest()\n",
    "        \n",
    "        name_test , result,expected = Test_BatchLoader.test_updateTreatedFiles()\n",
    "        test_setDiff = Test(name_test,result,expected,(lambda x,y : sorted(x)==sorted(y)))\n",
    "        test_setDiff.print_resultTest()\n",
    "        \n",
    "        \n",
    "        name_test , result,expected = Test_BatchLoader.test_updateNotTreatedFiles()\n",
    "        test_setDiff = Test(name_test,result,expected,(lambda x,y : sorted(x)==sorted(y)))\n",
    "        test_setDiff.print_resultTest()\n",
    "        \n",
    "        name_test , result,expected =Test_BatchLoader.test_getNextBatch()\n",
    "        test_setDiff = Test(name_test,result,expected)\n",
    "        test_setDiff.print_resultTest()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_leftSetDiff():\n",
    "        lst_1 = [\"2_113_12-6_1681562526.json\",\"1_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "        lst_2 = [\"2_113_12-6_1681562526.json\",\"9_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "        return \"leftSetDiff\" , BatchLoaderFiles.left_setDiff(lst_1,lst_2),[\"1_114_12-6_1681562526.json\"]\n",
    "    \n",
    "    \n",
    "    def rm_ifExist(str_path):\n",
    "\n",
    "        path_files = Path(str_path)\n",
    "        if path_files.exists() and path_files.is_dir():\n",
    "            shutil.rmtree(path_files)\n",
    "        return path_files\n",
    "    \n",
    "            \n",
    "    def test_getCurFiles():\n",
    "        str_path =os.path.join(os.getcwd(),\"tmp\")\n",
    "        lst_files = [\"2_113_12-6_1681562526.json\",\"9_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "        path_files = Path(str_path)\n",
    "        Mock_BatchLoader.delete_content_files( str_path)\n",
    "        Mock_BatchLoader.mock_create_files(str_path,lst_files)\n",
    "        batch_loader = BatchLoaderFiles( os.path.join(os.getcwd(),str_path))\n",
    "        return \"getCurFiles\" , batch_loader.getCurFiles (),lst_files\n",
    "    \n",
    "    def test_updateTreatedFiles():\n",
    "        str_path =os.path.join(os.getcwd(),\"tmp\")\n",
    "        lst_files_1 = [\"2_113_12-6_1681562526.json\",\"9_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "       \n",
    "        Mock_BatchLoader.delete_content_files(str_path)\n",
    "        Mock_BatchLoader.mock_create_files(str_path,lst_files_1)\n",
    "        \n",
    "        btlf= BatchLoaderFiles(str_path)\n",
    "        btlf.updateNotTreatedFiles()\n",
    "        \n",
    "        btlf.updateTreatedFiles(lst_files_1[:-2])\n",
    "        \n",
    "        return \"updateTreatedFiles\",btlf.treated_files,lst_files_1[:-2]\n",
    "    \n",
    "    def test_updateNotTreatedFiles():\n",
    "        \n",
    "        str_path =os.path.join(os.getcwd(),\"tmp\")\n",
    "        lst_files_1 = [\"2_113_12-6_1681562526.json\",\"9_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "       \n",
    "        Mock_BatchLoader.delete_content_files(str_path)\n",
    "        Mock_BatchLoader.mock_create_files(str_path,lst_files_1)\n",
    "        \n",
    "        btlf= BatchLoaderFiles(str_path)\n",
    "        btlf.updateNotTreatedFiles()\n",
    "        \n",
    "        \n",
    "        btlf.updateTreatedFiles(lst_files_1[:-2])\n",
    "        \n",
    "        \n",
    "        \n",
    "        lst_files_2 = [\"2_403_12-6_1681562526.json\",\"9_140_12-6_1681562526.json\",\"1_001_12-6_1681562526.json\",\"6_119_12-6_1681562526.json\"]\n",
    "       \n",
    "        Mock_BatchLoader.mock_create_files(str_path,lst_files_2)\n",
    "        btlf.updateNotTreatedFiles()\n",
    "        \n",
    "        return \"updateNotTreatedFiles\",btlf.not_treatedFiles,lst_files_2 + lst_files_1[-2:]\n",
    "        \n",
    "        \n",
    "    def test_getNextBatch():\n",
    "        str_path =os.path.join(os.getcwd(),\"tmp\")\n",
    "        lst_files_1 = [\"2_113_12-6_1681562526.json\",\"9_114_12-6_1681562526.json\",\"1_111_12-6_1681562526.json\",\"6_110_12-6_1681562526.json\"]\n",
    "       \n",
    "        Mock_BatchLoader.delete_content_files(str_path)\n",
    "        Mock_BatchLoader.mock_create_files(str_path,lst_files_1)\n",
    "        \n",
    "        btlf= BatchLoaderFiles(str_path,batch_len =2)\n",
    "        batch_1 = btlf.getNextBatch()\n",
    "        btlf.updateTreatedFiles(btlf.getNextBatch())\n",
    "        \n",
    "        \n",
    "        batch_2 = btlf.getNextBatch()\n",
    "        btlf.updateTreatedFiles(btlf.getNextBatch())\n",
    "        \n",
    "        \n",
    "        return \"getNextBatch\",sorted(batch_2)!=sorted(batch_1) ,True\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e127e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST leftSetDiff PASSED \n",
      "TEST getCurFiles PASSED \n",
      "TEST updateTreatedFiles PASSED \n",
      "TEST updateNotTreatedFiles PASSED \n",
      "TEST getNextBatch PASSED \n"
     ]
    }
   ],
   "source": [
    "test_batch =Test_BatchLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceced90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "from myutil  import *\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "'''\n",
    "ReadActions :\n",
    "    - STATIC :\n",
    "        - MAP_SIZE : nombre de cases sur la map\n",
    "        - intervals_ends : fin des intervalles des types d'actions ( e.g Times : 100 ,  Buffs: 200 , ... )\n",
    "        - intervals_dict : dictionnaire inversé de intervals_dict ( e.g { {START_FIGHT: 0 ,...,USE_WEAPON: 16  , LOST_PT : 100 , ... , NOVA_VITALITY :  112 , ... }  }# le fichier filename_dfActions loader \n",
    "        - intervals_dictRev : dictionnaire des types d'actions ( e.g {0:START_FIGHT ,...,16: USE_WEAPON , ... } # le fichier filename_dfActions loader reversed\n",
    "        - filename_dfActions : nom du fichier json contenant les actions de chaque types d'actions ainsi que la taille des intervalles \n",
    "        - ex_idx : liste des index des membres du json  a exclure (e.g [0,2,3]<=> interval_size , Funs , Others )\n",
    "        - lst_col : liste des colonnes non calculer (directement accessible via l'outcome ) à stocker dans le dataframe\n",
    "        - id_col : liste des colonnes contenant les ids identifiants les lignes du dataframe\n",
    "        - deg_col : liste des colonnes contenant les degats infligés ou subis par les entités (parmis les colonnes décritent dans lst_col)\n",
    "        - audit_col : liste des types d'actions performer pendant le tour (e.g [0,0,1,2,1,1] <=> [df.cols[0] , df.cols[0], df.cols[1] , ...] <=> [Times , Times , Buffs , ...])\n",
    "\n",
    "    - NON STATIC :\n",
    "        - numGen : numéro de la génération\n",
    "        - numFight : numéro du combat\n",
    "        - beg_notCodeAct_col : index indiquant le début des colonnes qui ne sont pas des liste de codes d'actions\n",
    "        - entities_ids : les ids réelle des entités (telle que spécifié dans le scénario[\"entities\"][i][\"id\"] ) impliquées dans le fight du dataframe \n",
    "        - int_col_idx_in_lst_col : liste des index des colonnes contenant des ints dans lst_col \n",
    "\n",
    "'''\n",
    "class ReadActions :\n",
    "    MAP_SIZE = 612 \n",
    "    intervals_ends = dict()\n",
    "    intervals_dict = dict()\n",
    "    intervals_dictRev = dict()  \n",
    "    filename_dfActions = os.path.join(\"..\",\"data\",\"df_Action.json\")\n",
    "    filename_dfValueActions = os.path.join(\"..\",\"data\",\"df_value_Action.json\")\n",
    "    \n",
    "    dict_dfValue = json.load(open( filename_dfValueActions))\n",
    "    ex_idx = [0,2,3]\n",
    "    lst_col = ['LOST_LIFE','LIFE','MOVE_TO','WEAPON_DEG','CHIP_DEG','MAPS','WEAPON_ID','CHIP_ID']\n",
    "    _cols = ['MOVE_TO','MAPS','WEAPON_ID','CHIP_ID']\n",
    "    id_col = ['ID_GEN','ID_FIGHT','ID_ENTITY','ID_TURN']\n",
    "    deg_col = ['LOST_LIFE','LIFE','WEAPON_DEG','CHIP_DEG']\n",
    "    audit_col = [\"ACTIONS\"]\n",
    "    \n",
    "    #maps = [ [0]*MAP_SIZE ,[0]*MAP_SIZE]\n",
    "    \n",
    "    def __init__(self,ids :list ,numGen :int , numFight:int ,path_atm :str ):\n",
    "        self.numGen = numGen\n",
    "        self. numFight = numFight\n",
    "        self.numTurn = 0\n",
    "        ReadActions.init_intervals(ReadActions.filename_dfActions)#,ReadActions.ex_idx)\n",
    "        self.nb_type = len(ReadActions.intervals_ends)\n",
    "        self.lst_col_code = [list(ReadActions.intervals_ends.keys())[i] for i in range(1,len(ReadActions.intervals_ends)) ] \n",
    "        self.df = pd.DataFrame(columns=self.lst_col_code+ReadActions.lst_col+ReadActions.id_col+ReadActions.audit_col)\n",
    "        self.beg_notCodeAct_col = len(self.df.columns) - len(ReadActions.lst_col)-len(ReadActions.id_col)-1\n",
    "        self.entities_ids = ids \n",
    "        self.setIntCol()\n",
    "        \n",
    "    def setDF(self,df2):\n",
    "        if(df2.columns == self.df.columns):\n",
    "            self.df = pd.concat([self.df , df2])\n",
    "            \n",
    "        \n",
    "    def setIntCol(self):\n",
    "        self.int_col_idx_in_lst_col = []\n",
    "        for i,d in enumerate(self.df.columns.tolist()) :\n",
    "            if d in ReadActions.deg_col :\n",
    "                self.int_col_idx_in_lst_col+=[i]\n",
    "        self.int_col_idx_in_lst_col.sort()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def getNumGen(self):\n",
    "        return self.numGen\n",
    "    \n",
    "    @property\n",
    "    def getNumFight(self):\n",
    "        return self.numFight\n",
    "    \n",
    "    @property\n",
    "    def getNumTurn(self):\n",
    "        return self.numTurn\n",
    "    \n",
    "    @property\n",
    "    def getDF(self):\n",
    "        return self.df\n",
    "    \n",
    "    @property\n",
    "    def getEntitiesIds(self):\n",
    "        return self.entities_ids\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def emptyIntervalsEnd():\n",
    "            ReadActions.intervals_ends.update({'BEGIN':0})\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_intervals(filename_json_dfA :str ,ex_idx=[]):\n",
    "            \"\"\"\n",
    "            filename_json_dfA : fichier json contenant les actions de chaque types d'actions ainsi que la taille des intervalles\n",
    "            ex_idx : liste des index des membres du json  a exclure (e.g [0,2,3]<=> interval_size , Funs , Others )\n",
    "\n",
    "            Description : initialise les attributs intervals_dict , intervals_dictRev , intervals_ends de la classe ReadActions grâce au fichier json passer en paramètre\n",
    "            \"\"\"\n",
    "            if( isJsonFile( filename_json_dfA ) ):\n",
    "                ReadActions.emptyIntervalsEnd()\n",
    "                #Load json file\n",
    "                with open(filename_json_dfA, 'r') as f:\n",
    "                    df_actions_json = json.load(f)\n",
    "                interval_size = df_actions_json[\"interval_size\"]\n",
    "\n",
    "                #ignore interval_size from dictionnary\n",
    "                del df_actions_json[\"interval_size\"]\n",
    "\n",
    "                #ignore ex_idx from dictionnary\n",
    "                for to_ignore in ex_idx :\n",
    "                    del df_actions_json[to_ignore]\n",
    "\n",
    "                #extract info from dict and load it in class attributes \n",
    "\n",
    "                idx_keyOf_action = 0\n",
    "                idx_intcodeOf_action = 1\n",
    "                for i,group_of_actions in enumerate(df_actions_json.items())  :\n",
    "                    id_group,lst_actions=group_of_actions\n",
    "                    \n",
    "                    for action in lst_actions.items():\n",
    "                        ReadActions.intervals_dict.update({action[idx_keyOf_action]:action[idx_intcodeOf_action]})\n",
    "\n",
    "                    ReadActions.intervals_ends.update({id_group:(interval_size+1)*(i+1)})\n",
    "                ReadActions.intervals_dictRev = reversedict(ReadActions.intervals_dict)\n",
    "            else :\n",
    "                errorMessagePyth(\"file\"+filename_json_dfA+\" must be a json file\")\n",
    "        \n",
    "     \n",
    "    \n",
    "    @staticmethod\n",
    "    def whileToken(str_token : str , lst_actions :list ):\n",
    "        \"\"\"\n",
    "        str_token : le string correspondant au token a chercher dans lst_actions ( un token est exprimer sous la forme d'un int dans lst_actions)\n",
    "        lst_actions : liste d'actions  a parcourir   ( lst_action[i][0] == code_token ) \n",
    "\n",
    "        Description : parcours lst_actions jusqu'a ce que le token soit trouvé ou que la fin de la liste soit atteinte \n",
    "        Return : l'index de l'action token dans lst_actions tel que ( lst_action[index][0] == code_token )  ou -1 si le token n'a pas été trouvé\n",
    "        \"\"\"\n",
    "        for i in range(len(lst_actions)) :\n",
    "            action = lst_actions[i]\n",
    "            if(action[0] == ReadActions.intervals_dict.get(str_token)):\n",
    "                i+=1\n",
    "                return i\n",
    "        return notFound()\n",
    "    \n",
    "\n",
    "    @staticmethod        \n",
    "    def isEndFight(act_token: int):\n",
    "        return act_token == ReadActions.intervals_dict.get(\"END_FIGHT\")\n",
    "    \n",
    "    @staticmethod \n",
    "    def isBeginFight(act_token: int):\n",
    "        return act_token == ReadActions.intervals_dict.get(\"START_FIGHT\")\n",
    "    \n",
    "    @staticmethod \n",
    "    def isBeginTurn(act_token: int):\n",
    "        return act_token == ReadActions.intervals_dict.get(\"NEW_TURN\")\n",
    "    \n",
    "    @staticmethod \n",
    "    def isEndTurn(act_token: int):\n",
    "        return ReadActions.isEndFight(act_token) or ReadActions.isBeginTurn(act_token ) or act_token == ReadActions.intervals_dict.get(\"END_TURN\")\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def getDfLstTypeActions():\n",
    "        return list([[]])\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDfLstAudit():\n",
    "        return list([[]])\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDfLstInLstCol():\n",
    "        return [0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDfLstNotInLstCol():\n",
    "        return []\n",
    "    \n",
    "\n",
    "    def getNewRow(self):\n",
    "        lst =[]\n",
    "\n",
    "        #-----------------Type Actions [Times,Buffs , ... ] -----------------\n",
    "        idx = 0 \n",
    "        idx_end = self.beg_notCodeAct_col \n",
    "        #Type of actions , e.g Times,Buffs,Funs,Effects , Others \n",
    "        for i in range (idx,idx_end):\n",
    "            lst.append(ReadActions.getDfLstTypeActions())\n",
    "\n",
    "        t=0\n",
    "        #----------------- Lst Col -----------------\n",
    "        idx = idx_end\n",
    "        idx_end +=len(ReadActions.lst_col)\n",
    "        for i in range (idx , idx_end):\n",
    "            if i in self.int_col_idx_in_lst_col[t:] :\n",
    "                lst.append(ReadActions.getDfLstInLstCol())\n",
    "                t+=1\n",
    "            #elif ReadActions.lst_col[i-idx] in [\"WEAPON_ID\",\"CHIP_ID\"]:\n",
    "            #    lst.append(notSetId())\n",
    "            else :\n",
    "                lst.append(ReadActions.getDfLstNotInLstCol())\n",
    "\n",
    "        #----------------- Not in Lst Col -----------------\n",
    "        idx = idx_end\n",
    "        idx_end += len(ReadActions.id_col)\n",
    "        for i in range (idx ,  idx_end):\n",
    "            lst.append(notSetId())\n",
    "\n",
    "        #----------------- Audit -----------------\n",
    "        idx = idx_end\n",
    "        idx_end += len(ReadActions.audit_col)\n",
    "        for i in range (idx,idx_end):\n",
    "            lst.append(ReadActions.getDfLstAudit())#actions\n",
    "\n",
    "\n",
    "\n",
    "        return lst\n",
    "    \n",
    "    def idxNextRow(self):\n",
    "        return self.getCurrentIDX()+1\n",
    "    \n",
    "    def addNewRow(self):\n",
    "        self.df.loc[self.idxNextRow()]=self.getNewRow()\n",
    "    \n",
    "        \n",
    "    def addNewTurn(self,idx_turn: int):\n",
    "        \"\"\"\n",
    "        idx_turn : index du tour a ajouter\n",
    "        Description : ajoute une nouvelle ligne et set sont indexe de tour à idx_turn\n",
    "        \"\"\"\n",
    "        self.addNewRow()\n",
    "        #self.numTurn = idx_turn\n",
    "        self.setColumn ('ID_TURN', idx_turn )\n",
    "        \n",
    "    def getCurrentIDX (self):\n",
    "        return len(self.df)-1\n",
    "\n",
    "    def setColumn ( self , str_col:str ,val):\n",
    "        \"\"\"\n",
    "        Description : set la colonne str_col de la ligne courante à val\n",
    "        \"\"\"\n",
    "        if ( strColInCols(str_col ,self.df)):\n",
    "            self.df.loc[self.getCurrentIDX(),str_col]= val \n",
    "    \n",
    "\n",
    "    def addToColumn(self,str_col:str,val,pos:int=0):\n",
    "        \"\"\"\n",
    "        val : valeur a ajouter a la liste de la colonne str_col de la ligne courante\n",
    "        pos : position de la valeur a ajouter dans la liste de la colonne str_col de la ligne courante (par defaut 0)\n",
    "\n",
    "        Description : ajoute une valeur a la liste de la colonne str_col de la ligne courante\n",
    "        \"\"\"\n",
    "        if (strColInCols(str_col ,self.df)):\n",
    "            lst=self.df.loc[self.getCurrentIDX(),str_col]\n",
    "\n",
    "            if( len(lst )< pos):\n",
    "                errorMessagePyth(self,\"addToColumn\" , \"index out of range\")\n",
    "\n",
    "            if( not(bool(lst)) or len(lst )== pos):\n",
    "                lst+=[0]\n",
    "            \n",
    "            lst[pos]+= val  \n",
    "\n",
    "        return pos \n",
    "            \n",
    "            \n",
    "    def addToColumnList(self,str_col:str,val,pos:int=0):\n",
    "        \"\"\"\n",
    "        Attention : on embedde la valeur dans une liste car la colonne str_col est de type liste \n",
    "        \"\"\"\n",
    "        if(len(self.df[str_col]) == 0  or not(isinstance(self.df[str_col][0],list))):\n",
    "            errorMessagePyth(self,\"addToColumnList\" , \"column \"+str_col+\" must be a list\")\n",
    "            return badInsert()\n",
    "        else :\n",
    "            return self.addToColumn(str_col , [val],pos) \n",
    "\n",
    "    def extendColum(self,str_col:str,val:list):\n",
    "        \"\"\"\n",
    "        Description : on ajoute une liste a la liste de la colonne str_col de la ligne courante\n",
    "        \"\"\"\n",
    "        if (strColInCols(str_col ,self.df)):\n",
    "            self.df.loc[self.getCurrentIDX(),str_col].extend(val)\n",
    "       \n",
    "\n",
    "    def addToAudit(self , idx:int , idx_of_actionType :int ,   act_token:int  ):\n",
    "        \"\"\"\n",
    "        Description : on ajoute l'index du type de l'action courante dans la colonne audit correspondante\n",
    "        \"\"\"\n",
    "        err_code=self.addToColumnList (  ReadActions.audit_col[idx],idx_of_actionType)\n",
    "        if(err_code == badInsert()):\n",
    "            errorMessagePyth(self, \"addToCategory\" , \"bad insert\")\n",
    "            \n",
    "    def addToGroupAction(self,idx_of_actionType:int , act_token:int):\n",
    "        err_code = self.addToColumnList (list(ReadActions.intervals_ends.keys())[idx_of_actionType],act_token)\n",
    "        if(err_code == badInsert()):\n",
    "            errorMessagePyth(self, \"addToCategory\" , \"bad insert\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def setIDs(self,entity_id:int):\n",
    "        \"\"\"\n",
    "        Description : set les colonnes IDs de la ligne courante sauf ID_TURN \n",
    "        \"\"\"\n",
    "        self.setColumn ('ID_GEN',self.numGen)\n",
    "        self.setColumn ('ID_FIGHT',self. numFight)\n",
    "        self.setColumn ('ID_ENTITY',self.getEntityID(entity_id))\n",
    "        #self.setColumn ('ID_TURN',self.idx_turn)\n",
    "        \n",
    "    def getEntityID(self ,entity_id:int ):\n",
    "        \"\"\"\n",
    "        Description : retourne l'ID de l'entité entity_id si elle existe sinon retourne outOfRange()\n",
    "        \"\"\"\n",
    "        if len(self.entities_ids) > entity_id :\n",
    "            return self.entities_ids[entity_id]\n",
    "        \n",
    "        else :\n",
    "            errorMessagePyth(self,\"getEntityID\" , \"index out of range\")\n",
    "            return outOfRange()\n",
    "        \n",
    "    @staticmethod\n",
    "    def getIdxCodeToken():\n",
    "        return 0\n",
    "    @staticmethod\n",
    "    def getIdxEntityToken():\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def getCodeToken(_lst :list):\n",
    "        return _lst[ReadActions.getIdxCodeToken()]\n",
    "    @staticmethod\n",
    "    def getEntityToken(_lst :list):\n",
    "        return _lst[ReadActions.getIdxEntityToken()]\n",
    "    \n",
    "    def getTurn( self,lst_actions :list ,idx_turn :int ):\n",
    "        entity_id = -1 \n",
    "        \n",
    "        # continue jusqu'au token LEEK_TURN\n",
    "        nbActionRead=ReadActions.whileToken(\"LEEK_TURN\",lst_actions)\n",
    "\n",
    "        k = nbActionRead\n",
    "\n",
    "        if(k==notFound()):\n",
    "            return k , None \n",
    "        \n",
    "        act_token = ReadActions.getCodeToken(lst_actions[k])\n",
    "        entity_id =  ReadActions.getEntityToken(lst_actions[k-1])\n",
    "        \n",
    "        self.addNewTurn(idx_turn)\n",
    "        self.setIDs(entity_id)\n",
    "\n",
    "        #si le tour est vide : \n",
    "        if(self.isEndTurn(act_token)):\n",
    "            return k , entity_id \n",
    "        \n",
    "        idx=0\n",
    "        while k  < len(lst_actions):\n",
    "\n",
    "            act_token = ReadActions.getCodeToken(lst_actions[k])\n",
    "\n",
    "            #si le tour est fini : \n",
    "            if(self.isEndTurn(act_token) ):\n",
    "                return k , entity_id\n",
    "            \n",
    "\n",
    "            for i in range(1,len(ReadActions.intervals_ends)):\n",
    "                #idx_group_actions=i\n",
    "                interval = [list(ReadActions.intervals_ends.values())[i-1],list(ReadActions.intervals_ends.values())[i]]\n",
    "                if(act_token  in ReadActions.intervals_dictRev and isInInterval(act_token , interval )) :\n",
    "                    \n",
    "                    _id = ReadActions.getEntityToken(lst_actions[k])\n",
    "                    self.addToAudit(ReadActions.audit_col.index(\"ACTIONS\"),i,act_token)\n",
    "                    self.addToGroupAction(i , act_token)\n",
    "\n",
    "        \n",
    "\n",
    "                    if(list(ReadActions.intervals_ends.keys())[i] == \"Times\"):\n",
    "\n",
    "                        if(ReadActions.intervals_dictRev.get(act_token) == \"MOVE_TO\"):\n",
    "                            lst_mv = lst_actions[k][ReadActions.dict_dfValue[str(act_token)][\"name_fields\"].index(\"path\")]\n",
    "                            self.extendColum('MOVE_TO',[lst_mv[0],len(lst_mv),lst_mv[-1]])\n",
    "                            self.extendColum('MAPS',lst_mv)\n",
    "                            \n",
    "  \n",
    "                            \n",
    "                    elif(list(ReadActions.intervals_ends.keys())[i] == \"Buffs\"):\n",
    "                        if(ReadActions.intervals_dictRev.get(act_token) == 'HEAL'):\n",
    "                            self.addToColumn('LIFE',lst_actions[k][ReadActions.dict_dfValue[str(act_token)][\"name_fields\"].index(\"life\")])\n",
    "                        elif(ReadActions.intervals_dictRev.get(act_token) in ['LOST_LIFE','NOVA_DAMAGE','DAMAGE_RETURN','LIFE_DAMAGE','POISON_DAMAGE',' AFTEREFFECT']):\n",
    "                            self.addToColumn('LOST_LIFE',lst_actions[k][ReadActions.dict_dfValue[str(act_token)][\"name_fields\"].index(\"pv\")])\n",
    "                    elif(list(ReadActions.intervals_ends.keys())[i] == \"Effects\"):\n",
    "                        if(ReadActions.intervals_dictRev.get(act_token) == 'ADD_WEAPON_EFFECT'):\n",
    "                            self.addToColumn('WEAPON_DEG',lst_actions[k][ReadActions.dict_dfValue[str(act_token)][\"name_fields\"].index(\"value\")])\n",
    "                            self.extendColum('WEAPON_ID',[_id])\n",
    "                        elif(ReadActions.intervals_dictRev.get(act_token) == 'ADD_CHIP_EFFECT'):\n",
    "                                self.addToColumn('CHIP_DEG',lst_actions[k][ReadActions.dict_dfValue[str(act_token)][\"name_fields\"].index(\"value\")])\n",
    "                                self.extendColum('CHIP_ID',[_id])\n",
    "                                \n",
    "                    \n",
    "                    \n",
    "            k+=1\n",
    "            idx+=1\n",
    "        return k , entity_id\n",
    "\n",
    "                            \n",
    "                    \n",
    "    def getFight ( self,lst_actions:list ):\n",
    "        nbReads=ReadActions.whileToken(\"START_FIGHT\",lst_actions)\n",
    "\n",
    "        k=nbReads\n",
    "\n",
    "        idx_turn = 0 \n",
    "        while k < len(lst_actions):\n",
    "            act_token =ReadActions.getCodeToken(lst_actions[k])\n",
    "            if(self.isEndFight(act_token )):\n",
    "                return k \n",
    "            nbReads , ent_id = self.getTurn(lst_actions[k:],idx_turn)\n",
    "            if(nbReads==-1):\n",
    "                break\n",
    "            k+=(nbReads+1)\n",
    "            if(k < len(lst_actions)):\n",
    "                act_token =ReadActions.getCodeToken(lst_actions[k])\n",
    "                if(self.isBeginTurn(act_token)):\n",
    "                    idx_turn +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d768f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fileOutcome_retDF(example_1_outcome,pathOutcome,quatuor=None,ancien=False):  \n",
    "        if quatuor is None and b_filename_isvalid(example_1_outcome):\n",
    "            quatuor= filename_isvalid(example_1_outcome)  \n",
    "        \n",
    "        if(len(quatuor) == 4):\n",
    "            numGen , numFight , ids_str , _example_1_outcome = quatuor\n",
    "            ids = splitIds(ids_str)\n",
    "            rda = ReadActions(ids ,numGen, numFight , _example_1_outcome )\n",
    "\n",
    "            example_1_outcome_path = os.path.join(pathOutcome , example_1_outcome)\n",
    "\n",
    "            dct_outcome = dict()\n",
    "            with open(example_1_outcome_path) as _f2:\n",
    "                dct_outcome = json.load(_f2)\n",
    "            if(isinstance(dct_outcome,str)):\n",
    "                dct_outcome =json.loads(dct_outcome )\n",
    "            lst_actions =None \n",
    "            if(\"fight\" in dct_outcome):\n",
    "                dct_outcome = dct_outcome[\"fight\"]\n",
    "            if(\"actions\" in dct_outcome):\n",
    "                lst_actions = dct_outcome[\"actions\"]\n",
    "            #lst_actions = dct_outcome[\"actions\"]if ancien else dct_outcome[\"fight\"][\"actions\"]\n",
    "            if(bool(lst_actions )):\n",
    "                rda.getFight (lst_actions)\n",
    "                return rda\n",
    "            else :\n",
    "                return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1900f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_atmLife(df_cpy,life_entities , str_col =\"ADD_LIFE\" ):\n",
    "    lst_res = []\n",
    "    nb_elem = len(life_entities)\n",
    "    for i , add_life in enumerate(df_cpy[str_col ]) :\n",
    "\n",
    "        if i >= nb_elem :\n",
    "                    lst_res.append(list(map(add,add_life,lst_res[-nb_elem])))\n",
    "\n",
    "        else:\n",
    "            lst_res.append(list(map(add,add_life,life_entities[len(lst_res)-nb_elem])))\n",
    "    return  lst_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd6abf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_addLife_atmLife(df , life_entities):\n",
    "    df_cpy = df.copy()\n",
    "    df_cpy[\"ADD_LIFE\"] = df_cpy.apply(lambda row: list( map(sub, row[\"LIFE\"] , row[\"LOST_LIFE\"])), axis=1)\n",
    "    df_cpy[\"ATM_LIFE\"]=lst_atmLife(df_cpy , life_entities)\n",
    "    return df_cpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a8b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_int_list(x):\n",
    "    return ast.literal_eval(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de98619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxID(str_col):\n",
    "    path_data = \"D:\\\\Master\\\\S2\\\\TER\\\\analyse\\\\data\"\n",
    "    if(str_col == \"MAPS\"):\n",
    "        return [i for i in range(612)]\n",
    "    if(str_col == \"WEAPON_ID\"):\n",
    "        dct_ = json.load(open(os.path.join(path_data,\"weapons\"+\".json\")))\n",
    "        return [e[\"item\"] for e in dct_.values()] #list(map(int,list(dct_.keys())))\n",
    "    if(str_col == \"CHIP_ID\"):\n",
    "        dct_ = json.load(open(os.path.join(path_data,\"chips\"+\".json\")))\n",
    "        return list(map(int,list(dct_.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad5f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_folder_genSplit(nb_folders_gen,nb_folders_turn,root_path =os.getcwd() ):\n",
    "    if not(os.path.exists(root_path)) or  not(os.path.isdir(root_path)):\n",
    "        os.makedirs(root_path)\n",
    "    for partGen in range(nb_folders_gen):\n",
    "        path_gen=os.path.join(root_path,str(partGen))\n",
    "        print(path_gen)\n",
    "        if not(os.path.exists(path_gen)) or  not(os.path.isdir(path_gen)):\n",
    "            os.mkdir(path_gen)\n",
    "        '''for partTurn in range(nb_folders_turn):\n",
    "            path_turn= os.path.join(path_gen,str(partTurn))\n",
    "            print(path_turn)\n",
    "            if not(os.path.exists(path_turn)) or  not(os.path.isdir(path_turn)):\n",
    "                os.mkdir(path_turn)'''\n",
    "    return root_path\n",
    "\n",
    "def clean_folder_genSplit(root_path):\n",
    "    if os.path.exists(root_path) and  os.path.isdir(root_path):\n",
    "        shutil.rmtree(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a999b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#dict_part={\"PART_TURN\":0,\"PART_GEN\":6}\n",
    "def df_cols (path_root_f_  ,dict_part,index_col_=[i for i in range(len(ReadActions.id_col))],ignore_col = [\"MOVE_TO\"]):\n",
    "\n",
    "    path_f= os.path.join(os.path.join(path_root_f_ ,str(dict_part[\"PART_GEN\"])),str(dict_part[\"PART_TURN\"]))\n",
    "    \n",
    "    dct_res=dict()\n",
    "    \n",
    "    for str_col_ in ReadActions._cols:\n",
    "\n",
    "        if(not(str_col_ in ignore_col )):\n",
    "            acc_counter = Counter()\n",
    "            f_name_out = os.path.join(path_f ,str_col_)+\".csv\"\n",
    "\n",
    "            df = pd.read_csv(f_name_out,index_col=index_col_,sep=\";\",encoding=\"utf-8\",converters={str_col_: convert_int_list})#DataFrame({'data': [np.random.randn(3) for i in range(N)]})\n",
    "\n",
    "            batch_size = 2500\n",
    "\n",
    "            for i in range(0, len(df), batch_size):\n",
    "\n",
    "                batch = df.loc[df.index[i:i+batch_size-1], str_col_].tolist()\n",
    "\n",
    "                batch_size = len(batch)\n",
    "\n",
    "\n",
    "                # Concaténation des listes\n",
    "                concatenated_data = np.concatenate(batch)\n",
    "                my_counter = Counter(concatenated_data)\n",
    "\n",
    "                acc_counter += my_counter\n",
    "\n",
    "            res_counter = Counter({i: 0 for i in getMaxID(str_col_)})#unecessary if skip nan after \n",
    "            res_counter.update(acc_counter)\n",
    "            res_counter.update(dict_part)\n",
    "            #res_counter.update({\"PART_GEN\":part_gen_})\n",
    "            \n",
    "            \n",
    "            res_counter = {str(k): v for k, v in res_counter.items()}\n",
    "            \n",
    "            \n",
    "            #path_f_csv = os.path.join(path_root_f_,str_col_+'.csv')\n",
    "            '''try:\n",
    "                df_write = pd.read_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\", index_col=[0])\n",
    "            except :\n",
    "                df_write = pd.DataFrame(columns=list(res_counter.keys()))\n",
    "            '''\n",
    "\n",
    "            \n",
    "            # Append the new row to the DataFrame\n",
    "            #df_write = df_write.append(res_counter,ignore_index=True)\n",
    "            \n",
    "\n",
    "            \n",
    "            #df_dct_ = pd.DataFrame([res_counter])\n",
    "            \n",
    "            #df_write =pd.concat([df_write, df_dct_],ignore_index=True)\n",
    "            \n",
    "            \n",
    "            #display(df_write)\n",
    "            \n",
    "\n",
    "            # Write the DataFrame back to the CSV file\n",
    "            #df_write.to_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\")\n",
    "            dct_res.update({str_col_ : res_counter})\n",
    "    return dct_res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bbd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_deg_col (path_root_f_  ,dict_part,index_col_=[i for i in range(len(ReadActions.id_col))]):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        path_f= os.path.join(os.path.join(path_root_f_,str(dict_part[\"PART_GEN\"])),str(dict_part[\"PART_TURN\"]))\n",
    "        \n",
    "        dct_res = dict()\n",
    "        \n",
    "        for str_col_ in ReadActions.deg_col:\n",
    "\n",
    "            f_name_out = os.path.join(path_f ,str_col_)+\".csv\"\n",
    "\n",
    "            df = pd.read_csv(f_name_out,index_col=index_col_,sep=\";\",encoding=\"utf-8\",converters={str_col_: convert_int_list})#DataFrame({'data': [np.random.randn(3) for i in range(N)]})\n",
    "\n",
    "\n",
    "            # Définition de la taille des batchs\n",
    "            batch_size = 2500\n",
    "\n",
    "            # Initialisation des accumulateurs\n",
    "            mean_acc = 0.0\n",
    "            variance_acc = 0.0\n",
    "            count = 0\n",
    "\n",
    "            # Boucle sur les batchs\n",
    "            for i in range(0, len(df), batch_size):\n",
    "\n",
    "                batch = df.loc[df.index[i:i+batch_size-1], str_col_].tolist()\n",
    "\n",
    "                batch_size = len(batch)\n",
    "\n",
    "\n",
    "                # Concaténation des listes\n",
    "                concatenated_data = np.concatenate(batch)\n",
    "                batch_mean = np.mean(concatenated_data)\n",
    "                batch_var = np.var(concatenated_data)\n",
    "                batch_count = len(concatenated_data)\n",
    "\n",
    "\n",
    "                # Mise à jour des accumulateurs\n",
    "                delta = batch_mean - mean_acc\n",
    "                mean_acc += delta * batch_count / (count + batch_count)\n",
    "                variance_acc += batch_var * batch_count + delta**2 * count * (batch_count / (count + batch_count))\n",
    "                count += batch_count\n",
    "\n",
    "            # Calcul de la moyenne, de la variance et de l'écart-type\n",
    "            mean = round(mean_acc,2)\n",
    "            _tu_ = variance_acc /(count - 1) if count > 1 else 0\n",
    "            variance = round(_tu_,2)\n",
    "            std_dev = round(np.sqrt(variance),2)\n",
    "            \n",
    "            res_counter = dict({\"MEAN\":mean,\"VAR\":variance,\"STD_DEV\":std_dev})\n",
    "            res_counter.update(dict_part)\n",
    "\n",
    "\n",
    "            #path_f_csv = os.path.join(path_root_f_,str_col_+'.csv')\n",
    "\n",
    "            '''try:\n",
    "                df_write = pd.read_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\", index_col=[0])\n",
    "            except :\n",
    "                df_write = pd.DataFrame(columns=list(dict_part.keys()))\n",
    "            '''\n",
    "                \n",
    "            # Append the new row to the DataFrame\n",
    "\n",
    "            \n",
    "            dct_res.update({str_col_ : res_counter})\n",
    "            \n",
    "            \n",
    "\n",
    "            #df_write = df_write.append(res_counter,ignore_index=True)\n",
    "            #df_write =pd.concat([df_write, pd.DataFrame([res_counter])],ignore_index=True)\n",
    "            #df_write.to_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\")\n",
    "    except  :\n",
    "        print(\"UnsortedIndexError\")\n",
    "        \n",
    "    return dct_res\n",
    "    \n",
    "    \n",
    "#display(df_stat_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d83aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_LIMIT = 3*pow(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb39f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treate_batch(dct_num,root_path_gen_turn,batch_,dict_len_part,nbPartTurn,life_entities,str_path=os.getcwd(),ignore_col = [\"MOVE_TO\"],index_col_=[i for i in range(len(ReadActions.id_col))]):\n",
    "    len_part_turn = dict_len_part[\"LEN_PART_TURN\"]\n",
    "    len_part_gen= dict_len_part[\"LEN_PART_GEN\"]\n",
    "    \n",
    "    dict_part={\"PART_GEN\":-1,\"PART_TURN\":-1}\n",
    "    lst_dct_deg_col = {key: [] for key in ReadActions.deg_col}\n",
    "    lst_lst_col  = {key: [] for key in ReadActions._cols if not (key  in ignore_col )}\n",
    "        \n",
    "    for filename_batch in batch_:    \n",
    "        if b_filename_isvalid(filename_batch):\n",
    "                    _quatuor= filename_isvalid(filename_batch)  \n",
    "        _rda = parse_fileOutcome_retDF(filename_batch,str_path,quatuor=_quatuor)\n",
    "        if(_rda is None ):\n",
    "            print(filename_batch)\n",
    "            print(\"ERREUR PARSE \")\n",
    "            continue\n",
    "        df_final=df_addLife_atmLife(_rda.df,life_entities)\n",
    "        dict_part[\"PART_GEN\"]=int(_quatuor[0])//len_part_gen\n",
    "        path_gen_csv = os.path.join(root_path_gen_turn,str(dict_part[\"PART_GEN\"]))\n",
    "        for iz in range(nbPartTurn):\n",
    "            dict_part[\"PART_TURN\"]=iz\n",
    "            partie = df_final.loc[(df_final['ID_TURN'] > len_part_turn*iz) & (df_final['ID_TURN'] < len_part_turn*(iz+1))]\n",
    "            if partie.shape[0] > 0 :\n",
    "                path_turn_csv = os.path.join(path_gen_csv,str(iz))\n",
    "                if not(Path(path_turn_csv).exists()):\n",
    "                    os.mkdir(path_turn_csv)\n",
    "                for str_col in list(set(partie.columns)- set(ReadActions.id_col)):\n",
    "                    path_csv_col = os.path.join(path_turn_csv,str_col)\n",
    "                    final_path_csv =path_csv_col+\".csv\"\n",
    "                    sub_partie = partie[[str_col]+ReadActions.id_col]\n",
    "                    sub_partie.set_index(ReadActions.id_col,inplace=True)\n",
    "\n",
    "                    if Path(final_path_csv).exists():\n",
    "\n",
    "                        concat_df = pd.read_csv(final_path_csv,sep=\";\", index_col=index_col_,encoding=\"utf-8\")\n",
    "                        final_df = pd.concat([concat_df,sub_partie])\n",
    "                        final_df.to_csv(final_path_csv,sep=\";\",encoding=\"utf-8\")\n",
    "                    else:\n",
    "                        sub_partie.to_csv(final_path_csv,sep=\";\",encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "                #print(dict_part)\n",
    "                \n",
    "                dct_deg = df_deg_col (root_path_gen_turn,dict_part,index_col_=index_col_)\n",
    "                for _k ,_v in dct_deg.items():\n",
    "                    lst_dct_deg_col[_k].append(_v)\n",
    "                    \n",
    "                dct_lst = df_cols(root_path_gen_turn,dict_part,index_col_=index_col_)\n",
    "                for _k ,_v in dct_lst.items():\n",
    "                    __k= int(_k) if _k.isnumeric() else _k\n",
    "                    lst_lst_col[str(__k)].append(_v)\n",
    "                    \n",
    "    for _k , _v in lst_dct_deg_col.items():\n",
    "        path_r_csv =  os.path.join(root_path_gen_turn,str(_k))\n",
    "        path_f_csv = os.path.join(path_r_csv,str(dct_num[_k])+'.csv')\n",
    "        try:\n",
    "            if(os.path.getsize(path_f_csv) > SIZE_LIMIT):\n",
    "                df_write = pd.DataFrame(columns=list(_v[0].keys()))#columns=[\"MEAN\",\"VAR\",\"STD_DEV\"]+list(dict_part.keys()))\n",
    "                dct_num[_k]+=1\n",
    "                path_f_csv = os.path.join(path_r_csv,str(dct_num(_k))+'.csv')\n",
    "            else:\n",
    "                df_write = pd.read_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\", index_col=[0])\n",
    "        except :\n",
    "            df_write = pd.DataFrame(columns=list(_v[0].keys()))#columns=[\"MEAN\",\"VAR\",\"STD_DEV\"]+list(dict_part.keys()))\n",
    "        df_write = df_write.append(_v, ignore_index=True)\n",
    "        df_write.to_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\")\n",
    "        \n",
    "    for _k , _v in lst_lst_col.items():\n",
    "        path_r_csv =  os.path.join(root_path_gen_turn,str(_k))\n",
    "        path_f_csv = os.path.join(path_r_csv,str(dct_num[_k])+'.csv')\n",
    "        try:\n",
    "            if(os.path.getsize(path_f_csv) > SIZE_LIMIT):\n",
    "                df_write = pd.DataFrame(columns=list(_v[0].keys()))#getMaxID(str(_k))+list(dict_part.keys()))\n",
    "                dct_num[_k]+=1\n",
    "                path_f_csv = os.path.join(path_r_csv,str(dct_num[_k])+'.csv')\n",
    "            else:\n",
    "                df_write = pd.read_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\", index_col=[0])\n",
    "        except :\n",
    "            df_write = pd.DataFrame(columns=list(_v[0].keys()))#getMaxID(str(_k))+list(dict_part.keys()))\n",
    "        df_write = df_write.append(_v, ignore_index=True)\n",
    "        df_write.to_csv(path_f_csv ,sep=\";\",encoding=\"utf-8\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1f37f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_files_byBatch(life_entities,str_path_outcome ,dict_len_part,nbPartTurn,root_path_gen_turn,len_batch = 100):\n",
    "    btlf= BatchLoaderFiles(str_path_outcome,batch_len =len_batch)\n",
    "    batch_ = btlf.getNextBatch()\n",
    "    count_f_outcome=0\n",
    "    count_batch_outcome=0\n",
    "    dct_num={k:0 for k in ReadActions.lst_col }\n",
    "    while(len(batch_) >= len_batch):\n",
    "        count_batch_outcome += 1\n",
    "        '''\n",
    "        for _filename in batch_ :\n",
    "            count_f_outcome+=1\n",
    "            if b_filename_isvalid(_filename):\n",
    "                numGen , numFight , ids_str , outcome_filename = filename_isvalid(_filename)  \n",
    "                ids = splitIds(ids_str)'''\n",
    "\n",
    "        print(batch_)\n",
    "        treate_batch(dct_num,root_path_gen_turn,batch_,dict_len_part,nbPartTurn,life_entities,str_path=str_path_outcome)        \n",
    "        btlf.updateTreatedFiles(batch_)\n",
    "        batch_ = btlf.getNextBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f1bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_resume_folder(path_root_f):\n",
    "    for str_col in ReadActions.lst_col+ReadActions.audit_col :\n",
    "        path_r_csv = os.path.join(path_root_f,str_col)\n",
    "        os.mkdir(path_r_csv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826c306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9b8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "genetic_path = \"D:\\\\Master\\\\S2\\\\TER\\\\analyse\\\\src\"\n",
    "\n",
    "pathOutcome = os.path.join(genetic_path,\"outcome\")\n",
    "pathGeneticScript =   os.path.join(genetic_path,\"genetic-script-data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INIT SETTING\n",
    "\n",
    "settings_path = os.path.join(pathGeneticScript,\"settings-genetic.json\")\n",
    "settings = json.load(open(settings_path))\n",
    "\n",
    "## CREATE DATAFRAME SETTING\n",
    "\n",
    "life_entities= [[settings[\"leekStats\"][\"life\"]],[settings[\"leekStats\"][\"life\"]]]\n",
    "\n",
    "\n",
    "## GENERATION  SETTING\n",
    "nbMaxRow = 250000\n",
    "nbPartTurn = 32\n",
    "\n",
    "len_population = settings[\"population\"]\n",
    "len_maxturn = settings[\"mapStats\"][\"max_turns\"]\n",
    "nbGeneration=settings[\"generations\"]\n",
    "\n",
    "combat_quantity=settings[\"combatsQuantity\"]\n",
    "\n",
    "## BATCH SOTRING SETTING\n",
    "\n",
    "\n",
    "len_part_turn = len_maxturn // nbPartTurn\n",
    "u_tmp = nbMaxRow //(len_part_turn*len_population*combat_quantity)\n",
    "nbPartGen = u_tmp if u_tmp <nbGeneration and   nbGeneration //u_tmp > 100  else 100\n",
    "len_part_gen =  nbGeneration//nbPartGen\n",
    "\n",
    "dict_len_part={\"LEN_PART_GEN\": len_part_gen, \"LEN_PART_TURN\":len_part_turn}\n",
    "\n",
    "\n",
    "\n",
    "## BATCH PARAM \n",
    "\n",
    "root_path_gen_turn = init_folder_genSplit(nbPartGen,nbPartTurn,os.path.join(os.getcwd(),\"outcome_gen_turn\"))\n",
    "\n",
    "N = 1000\n",
    "\n",
    "len_batch = 25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70601e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAUNCH \n",
    "\n",
    "init_resume_folder(root_path_gen_turn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3bfff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "treat_files_byBatch(life_entities,str_path_outcome=pathOutcome ,dict_len_part=dict_len_part,nbPartTurn=nbPartTurn,root_path_gen_turn=root_path_gen_turn,len_batch = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1[0]=\"31_4_12-62_1681316162306.json\"\n",
    "batch_1[1]=\"31_5_12-62_1681316162306.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0672414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len_part_gen*len_part_turn*len_population) # <=250000\n",
    "print(nbPartGen)\n",
    "print(root_path_gen_turn)\n",
    "print(len_part_gen)\n",
    "print(len_part_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_batch_outcome*len_batch)\n",
    "print(count_f_outcome)\n",
    "print (len([name for name in os.listdir(str_path) if os.path.isfile(os.path.join(str_path, name))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cpy[\"ATM_LIFE_2\"] = df_cpy[\"ATM_LIFE\"].rolling(window=2).apply(_funct_2 , raw=True)\n",
    "#df_cpy[\"ATM_LIFE\"].rolling(1).apply(lambda lst_lst : list( map(add,lst_lst[0],lst_lst[1])),axis=1)\n",
    "df_cpy.loc[len(df_cpy)-nb_elem*2::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911518bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd = dict({\"MEAN\":10.0,\"VAR\":2.2,\"STD_DEV\":1.01})\n",
    "ckd.update(dict_part)\n",
    "print(ckd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _funct_2 (x_lst):\n",
    "    _x = x_lst[0]\n",
    "    for x in x_lst[2::2] :\n",
    "          _x=list( map(add , _x , x))\n",
    "    return _x \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15380a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rda.dict_dfValue[str(10)][\"name_fields\"].index(\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloned_python_marche_env_2",
   "language": "python",
   "name": "cloned_python_marche_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
